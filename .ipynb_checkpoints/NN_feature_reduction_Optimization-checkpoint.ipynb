{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import project_functions2 as pf\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These top two lines are needed to produce altair plots on google colab\n",
    "# Comment these two lines out if you are running locally\n",
    "!pip install altair_data_server\n",
    "alt.data_transformers.enable('data_server')\n",
    "\n",
    "# These bottem two lines are needed tto produce altair plots on local machine\n",
    "# Comment these two line out to run on google colab\n",
    "#alt.renderers.enable('default')\n",
    "#alt.data_transformers.enable('json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock_list = ['AMZN', 'AAPL', 'FB','GOOGL', 'MSFT', 'TSLA']\n",
    "stock_list = ['AMZN', 'AAPL', 'FB','GOOG', 'TSLA']\n",
    "stock_objects = {}\n",
    "for stock in stock_list:\n",
    "    stock_objects[stock] = yf.Ticker(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sentiments = {}\n",
    "for key in stock_objects:\n",
    "    stock_sentiments[key] = pd.read_csv('/content/drive/MyDrive/SENG474_Project/data/sentiment/investing_'+key+'_sentiment.csv')\n",
    "    stock_sentiments[key].set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combiner(stock_dfs):\n",
    "    combine_df = None\n",
    "\n",
    "    for key in stock_dfs:\n",
    "        if combine_df is not None:\n",
    "            combine_df = pd.concat([combine_df, stock_dfs[key]])\n",
    "        else:\n",
    "            combine_df = stock_dfs[key]\n",
    "\n",
    "    combine_df.sort_values(by=['Date'], inplace=True)\n",
    "    return combine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction_nn(stock_objects, split_time):\n",
    "    scaler = MinMaxScaler()\n",
    "    drop_list = []\n",
    "    i = 4\n",
    "    \n",
    "    stock_dfs = {}\n",
    "    for key in stock_objects:\n",
    "        stock_dfs[key] = stock_objects[key].history(period='max')\n",
    "\n",
    "    for key in stock_dfs:\n",
    "        stock_dfs[key] = pf.date_time_prep(stock_dfs[key]) \n",
    "        stock_dfs[key] = pf.rolling_aves(stock_dfs[key])\n",
    "        stock_dfs[key] = stock_dfs[key].merge(stock_sentiments[key], how='left', left_index=True, right_index=True)\n",
    "        stock_dfs[key].fillna(0, inplace=True)\n",
    "        stock_dfs[key] = pf.future_close_setup(stock_dfs[key], 5)\n",
    "        \n",
    "    combine_df = pf.combiner(stock_dfs)\n",
    "    \n",
    "    X = combine_df.iloc[:,:-1]\n",
    "    y = combine_df.iloc[:,-1:]\n",
    "    \n",
    "    split_mark = int(len(combine_df)-(split_time*len(stock_dfs)))\n",
    "    X_train = X.head(split_mark)\n",
    "    X_test = X.tail(len(combine_df) - split_mark)\n",
    "    y_train = y.head(split_mark)\n",
    "    y_test = y.tail(len(combine_df) - split_mark)\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss',\n",
    "                                   patience=3, restore_best_weights=True)\n",
    "    \n",
    "    np.random.seed(45)\n",
    "    stock_model = Sequential()\n",
    "    stock_model.add(Dense(units=500, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "    stock_model.add(Dense(units=500, activation='relu'))\n",
    "    stock_model.add(Dense(units=500, activation='relu'))\n",
    "    stock_model.add(Dense(units=1, activation='relu'))\n",
    "    stock_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    stock_model.fit(X_train_scaled, y_train, epochs=60, batch_size=32, verbose=1,\n",
    "                    workers=-1, callbacks=[early_stopping])\n",
    "    stock_close_pred = stock_model.predict(X_test_scaled)\n",
    "    max_score = r2_score(y_test.dropna(), stock_close_pred[:len(y_test.dropna())])\n",
    "    \n",
    "    total_cols = len(combine_df.columns)\n",
    "\n",
    "    while i + len(drop_list) < total_cols:\n",
    "        print('Max Score: ' + str(max_score))\n",
    "        stock_dfs = {}\n",
    "        for key in stock_objects:\n",
    "            stock_dfs[key] = stock_objects[key].history(period='max')\n",
    "    \n",
    "        for key in stock_dfs:\n",
    "            stock_dfs[key] = pf.date_time_prep(stock_dfs[key]) \n",
    "            stock_dfs[key] = pf.rolling_aves(stock_dfs[key])\n",
    "            stock_dfs[key] = stock_dfs[key].merge(stock_sentiments[key], how='left', left_index=True, right_index=True)\n",
    "            stock_dfs[key].fillna(0, inplace=True)\n",
    "            stock_dfs[key] = pf.future_close_setup(stock_dfs[key], 5)\n",
    "            \n",
    "        combine_df = pf.combiner(stock_dfs)\n",
    "        \n",
    "        if len(drop_list) > 0:\n",
    "            combine_df.drop(drop_list, inplace=True, axis=1)\n",
    "            \n",
    "        curr_col = combine_df.columns[i]\n",
    "        combine_df.drop(curr_col, inplace=True, axis=1)\n",
    "            \n",
    "        X = combine_df.iloc[:,:-1]\n",
    "        y = combine_df.iloc[:,-1:]\n",
    "        \n",
    "        split_mark = int(len(combine_df)-(split_time*len(stock_dfs)))\n",
    "        X_train = X.head(split_mark)\n",
    "        X_test = X.tail(len(combine_df) - split_mark)\n",
    "        y_train = y.head(split_mark)\n",
    "        y_test = y.tail(len(combine_df) - split_mark)\n",
    "        \n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='loss',\n",
    "                                       patience=3, restore_best_weights=True)\n",
    "        \n",
    "        np.random.seed(45)\n",
    "        stock_model = Sequential()\n",
    "        stock_model.add(Dense(units=500, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "        stock_model.add(Dense(units=500, activation='relu'))\n",
    "        stock_model.add(Dense(units=500, activation='relu'))\n",
    "        stock_model.add(Dense(units=1, activation='relu'))\n",
    "        stock_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        stock_model.fit(X_train_scaled, y_train, epochs=60, batch_size=32, verbose=1,\n",
    "                        workers=-1, callbacks=[early_stopping])\n",
    "        stock_close_pred = stock_model.predict(X_test_scaled)\n",
    "        curr_score = r2_score(y_test.dropna(), stock_close_pred[:len(y_test.dropna())])\n",
    "        \n",
    "        if curr_score >= max_score:\n",
    "            drop_list.append(curr_col)\n",
    "            max_score = curr_score\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return max_score, drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score, drop_list = feature_reduction(stock_objects, 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_reduction2_nn(stock_objects, split_time):\n",
    "    scaler = MinMaxScaler()\n",
    "    drop_list = []\n",
    "    i = 0\n",
    "    drop_flag = True\n",
    "    drop_col = ''\n",
    "    \n",
    "    stock_dfs = {}\n",
    "    for key in stock_objects:\n",
    "        stock_dfs[key] = stock_objects[key].history(period='max')\n",
    "\n",
    "    for key in stock_dfs:\n",
    "        stock_dfs[key] = pf.date_time_prep(stock_dfs[key]) \n",
    "        stock_dfs[key] = pf.rolling_aves(stock_dfs[key])\n",
    "        stock_dfs[key] = stock_dfs[key].merge(stock_investing[key], how='left', left_index=True, right_index=True)\n",
    "        stock_dfs[key] = stock_dfs[key].merge(stock_stocks[key], how='left', left_index=True, right_index=True)\n",
    "        stock_dfs[key].fillna(0, inplace=True)\n",
    "        stock_dfs[key] = pf.future_close_setup(stock_dfs[key], 5)\n",
    "        \n",
    "    combine_df = pf.combiner(stock_dfs)\n",
    "    \n",
    "    X = combine_df.iloc[:,:-1]\n",
    "    y = combine_df.iloc[:,-1:]\n",
    "    \n",
    "    split_mark = int(len(combine_df)-(split_time*len(stock_dfs)))\n",
    "    X_train = X.head(split_mark)\n",
    "    X_test = X.tail(len(combine_df) - split_mark)\n",
    "    y_train = y.head(split_mark)\n",
    "    y_test = y.tail(len(combine_df) - split_mark)\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss',\n",
    "                                   patience=3, restore_best_weights=True)\n",
    "    \n",
    "    np.random.seed(45)\n",
    "    tf.random.set_seed(45)\n",
    "    stock_model = Sequential()\n",
    "    stock_model.add(Dense(units=500, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "    stock_model.add(Dense(units=500, activation='relu'))\n",
    "    stock_model.add(Dense(units=500, activation='relu'))\n",
    "    stock_model.add(Dense(units=1, activation='relu'))\n",
    "    stock_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "    stock_model.fit(X_train_scaled, y_train, epochs=60, batch_size=32, verbose=0,\n",
    "                    workers=-1, callbacks=[early_stopping])\n",
    "    stock_close_pred = stock_model.predict(X_test_scaled)\n",
    "    max_score = r2_score(y_test.dropna(), stock_close_pred[:len(y_test.dropna())])\n",
    "    \n",
    "    total_cols = len(combine_df.columns)\n",
    "\n",
    "    while drop_flag:\n",
    "        print('Max Score: ' + str(max_score))\n",
    "        print('Drop List: ' + str(drop_list))\n",
    "        i = 0\n",
    "        drop_flag = False\n",
    "        \n",
    "        while i + len(drop_list) < total_cols:\n",
    "            stock_dfs = {}\n",
    "            \n",
    "            for key in stock_objects:\n",
    "                stock_dfs[key] = stock_objects[key].history(period='max')\n",
    "        \n",
    "            for key in stock_dfs:\n",
    "                stock_dfs[key] = pf.date_time_prep(stock_dfs[key]) \n",
    "                stock_dfs[key] = pf.rolling_aves(stock_dfs[key])\n",
    "                stock_dfs[key] = stock_dfs[key].merge(stock_investing[key], how='left', left_index=True, right_index=True)\n",
    "                stock_dfs[key] = stock_dfs[key].merge(stock_stocks[key], how='left', left_index=True, right_index=True)\n",
    "                stock_dfs[key].fillna(0, inplace=True)\n",
    "                stock_dfs[key] = pf.future_close_setup(stock_dfs[key], 5)\n",
    "                \n",
    "            combine_df = pf.combiner(stock_dfs)\n",
    "            \n",
    "            if len(drop_list) > 0:\n",
    "                combine_df.drop(drop_list, inplace=True, axis=1)\n",
    "                \n",
    "            curr_col = combine_df.columns[i]\n",
    "            combine_df.drop(curr_col, inplace=True, axis=1)\n",
    "                \n",
    "            X = combine_df.iloc[:,:-1]\n",
    "            y = combine_df.iloc[:,-1:]\n",
    "            \n",
    "            split_mark = int(len(combine_df)-(split_time*len(stock_dfs)))\n",
    "            X_train = X.head(split_mark)\n",
    "            X_test = X.tail(len(combine_df) - split_mark)\n",
    "            y_train = y.head(split_mark)\n",
    "            y_test = y.tail(len(combine_df) - split_mark)\n",
    "            \n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            stock_model = Sequential()\n",
    "            stock_model.add(Dense(units=500, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "            stock_model.add(Dense(units=500, activation='relu'))\n",
    "            stock_model.add(Dense(units=500, activation='relu'))\n",
    "            stock_model.add(Dense(units=1, activation='relu'))\n",
    "            stock_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "            stock_model.fit(X_train_scaled, y_train, epochs=60, batch_size=32, verbose=0,\n",
    "                            workers=-1, callbacks=[early_stopping])\n",
    "            stock_close_pred = stock_model.predict(X_test_scaled)\n",
    "            curr_score = r2_score(y_test.dropna(), stock_close_pred[:len(y_test.dropna())])\n",
    "            \n",
    "            if curr_score >= max_score:\n",
    "                drop_col = curr_col\n",
    "                max_score = curr_score\n",
    "                drop_flag = True\n",
    "            \n",
    "            i += 1\n",
    "        drop_list.append(drop_col)\n",
    "    \n",
    "    return max_score, drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import project_functions2 as pf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list = ['AMZN', 'AAPL', 'FB','GOOGL', 'MSFT', 'TSLA']\n",
    "stock_objects = {}\n",
    "for stock in stock_list:\n",
    "    stock_objects[stock] = yf.Ticker(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_df, stock_dfs = pf.end_to_end_lstm_prep(stock_objects, lookback=30, pred_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs['FB'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split(combine_df, 365, stock_dfs)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 31, int(X_train.shape[1]/31)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 31, int(X_test.shape[1]/31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=500, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model.add(Dense(units = 1))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model.fit(X_train_3d, y_train, epochs=200, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_3d)\n",
    "len( predictions[:len(y_test.dropna())])\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_score = r2_score(y_test.dropna(), predictions[:len(y_test.dropna())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train_3d)\n",
    "model_score = r2_score(y_train, train_preds)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=2500, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model2.add(Dense(units = 1))\n",
    "model2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model2.fit(X_train_3d, y_train, epochs=200, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(X_test_3d)\n",
    "model_score2 = r2_score(y_test.dropna(), predictions[:len(y_test.dropna())])\n",
    "model_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2 = model2.predict(X_train_3d)\n",
    "model_train_score2 = r2_score(y_train, train_preds2)\n",
    "model_train_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "drop_list = ['Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'Mt',\n",
    "       '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "       '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "       '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "       '5 Day Volume Var', '5 Day High', '5 Day Low',\n",
    "       '10 Day Open Mean', '10 Day High Mean', '10 Day Low Mean',\n",
    "       '10 Day Close Mean', '10 Day Volume Mean', '10 Day Open Var',\n",
    "       '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "       '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "       '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "       '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "       '20 Day Volume Var', '20 Day High', '20 Day Low']\n",
    "clost_drop = ['Close', '- 1 Days Close', '- 2 Days Close', '- 3 Days Close',\n",
    "               '- 4 Days Close', '- 5 Days Close']\n",
    "for stock in stock_objects:\n",
    "    stock_dfs[stock] = stock_objects[stock].history(period='max')\n",
    "    stock_dfs[stock] = pf.rolling_aves(stock_dfs[stock])\n",
    "    #stock_dfs[stock].drop(drop_list, axis=1, inplace=True)\n",
    "    stock_dfs[stock] = pf.lstm_prep(stock_dfs[stock], lookback=5)\n",
    "    stock_dfs[stock] = pf.future_percent_change_setup(stock_dfs[stock], 5)\n",
    "    #stock_dfs[stock].drop(clost_drop, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "\n",
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split_scaled(combine_df, 365, stock_dfs)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 6, int(X_train.shape[1]/6)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 6, int(X_test.shape[1]/6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=500, return_sequences=True, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(units=500, return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(units=500))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(units = 1))\n",
    "model2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model2.fit(X_train_3d, y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(X_test_3d)\n",
    "model_score2 = r2_score(y_test.dropna(), predictions2[:len(y_test.dropna())])\n",
    "model_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds2 = model2.predict(X_train_3d)\n",
    "model_train_score2 = r2_score(y_train, train_preds2)\n",
    "model_train_score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "drop_list = ['Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'Mt',\n",
    "       '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "       '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "       '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "       '5 Day Volume Var', '5 Day High', '5 Day Low',\n",
    "       '10 Day Open Mean', '10 Day High Mean', '10 Day Low Mean',\n",
    "       '10 Day Close Mean', '10 Day Volume Mean', '10 Day Open Var',\n",
    "       '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "       '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "       '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "       '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "       '20 Day Volume Var', '20 Day High', '20 Day Low']\n",
    "clost_drop = ['Close', '- 1 Days Close', '- 2 Days Close', '- 3 Days Close',\n",
    "               '- 4 Days Close', '- 5 Days Close']\n",
    "for stock in stock_objects:\n",
    "    stock_dfs[stock] = stock_objects[stock].history(period='max')\n",
    "    stock_dfs[stock] = pf.rolling_aves(stock_dfs[stock])\n",
    "    stock_dfs[stock].drop(drop_list, axis=1, inplace=True)\n",
    "    stock_dfs[stock] = pf.lstm_prep(stock_dfs[stock], lookback=5)\n",
    "    stock_dfs[stock] = pf.future_percent_change_setup(stock_dfs[stock], 5)\n",
    "    stock_dfs[stock].drop(clost_drop, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "\n",
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split_scaled(combine_df, 365, stock_dfs)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 6, int(X_train.shape[1]/6)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 6, int(X_test.shape[1]/6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(LSTM(units=500, return_sequences=True, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(LSTM(units=500, return_sequences=True))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(LSTM(units=500))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(units = 1))\n",
    "model3.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model3.fit(X_train_3d, y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model3.predict(X_test_3d)\n",
    "model_score3 = r2_score(y_test.dropna(), predictions3[:len(y_test.dropna())])\n",
    "model_score3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds3 = model3.predict(X_train_3d)\n",
    "model_train_score3 = r2_score(y_train, train_preds3)\n",
    "model_train_score3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage Change Tests 5 and 30 day lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Close',\n",
       " 'Close in 1 Days',\n",
       " 'Close in 2 Days',\n",
       " 'Close in 3 Days',\n",
       " 'Close in 4 Days']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_drop = ['Close']\n",
    "for day in range(1,5):\n",
    "    close_drop.append('Close in ' + str(day) + ' Days')\n",
    "close_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "drop_list = ['Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'Mt',\n",
    "       '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "       '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "       '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "       '5 Day Volume Var', '5 Day High', '5 Day Low',\n",
    "       '10 Day Open Mean', '10 Day High Mean', '10 Day Low Mean',\n",
    "       '10 Day Close Mean', '10 Day Volume Mean', '10 Day Open Var',\n",
    "       '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "       '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "       '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "       '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "       '20 Day Volume Var', '20 Day High', '20 Day Low']\n",
    "close_drop = ['Close']\n",
    "for day in range(1,6):\n",
    "    close_drop.append('- ' + str(day) + ' Days Close')\n",
    "for stock in stock_objects:\n",
    "    stock_dfs[stock] = stock_objects[stock].history(period='max')\n",
    "    stock_dfs[stock] = pf.rolling_aves(stock_dfs[stock])\n",
    "    stock_dfs[stock].drop(drop_list, axis=1, inplace=True)\n",
    "    stock_dfs[stock] = pf.lstm_prep(stock_dfs[stock], lookback=5)\n",
    "    stock_dfs[stock] = pf.future_percent_change_setup(stock_dfs[stock], 5)\n",
    "    stock_dfs[stock].drop(close_drop, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "\n",
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split(combine_df, 365, stock_dfs)\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.fit_transform(y_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 6, int(X_train.shape[1]/6)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 6, int(X_test.shape[1]/6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "997/997 [==============================] - 177s 174ms/step - loss: 0.0084\n",
      "Epoch 2/100\n",
      "997/997 [==============================] - 176s 176ms/step - loss: 0.0026s - loss:\n",
      "Epoch 3/100\n",
      "997/997 [==============================] - 175s 176ms/step - loss: 0.0024\n",
      "Epoch 4/100\n",
      "997/997 [==============================] - 167s 168ms/step - loss: 0.0025\n",
      "Epoch 5/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0024\n",
      "Epoch 6/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0024\n",
      "Epoch 7/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0023\n",
      "Epoch 8/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0023\n",
      "Epoch 9/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0023\n",
      "Epoch 10/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0024s - l\n",
      "Epoch 11/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0023\n",
      "Epoch 12/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0023\n",
      "Epoch 13/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 14/100\n",
      "997/997 [==============================] - 162s 163ms/step - loss: 0.0023\n",
      "Epoch 15/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0023\n",
      "Epoch 16/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0023\n",
      "Epoch 17/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 18/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0022\n",
      "Epoch 19/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0022\n",
      "Epoch 20/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 21/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 22/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0023\n",
      "Epoch 23/100\n",
      "997/997 [==============================] - 162s 163ms/step - loss: 0.0022\n",
      "Epoch 24/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0023\n",
      "Epoch 25/100\n",
      "997/997 [==============================] - 162s 163ms/step - loss: 0.0022\n",
      "Epoch 26/100\n",
      "997/997 [==============================] - 185s 186ms/step - loss: 0.0023\n",
      "Epoch 27/100\n",
      "997/997 [==============================] - 178s 179ms/step - loss: 0.0022\n",
      "Epoch 28/100\n",
      "997/997 [==============================] - 182s 183ms/step - loss: 0.0022\n",
      "Epoch 29/100\n",
      "997/997 [==============================] - 182s 182ms/step - loss: 0.0022\n",
      "Epoch 30/100\n",
      "997/997 [==============================] - 179s 180ms/step - loss: 0.0022s\n",
      "Epoch 31/100\n",
      "997/997 [==============================] - 167s 167ms/step - loss: 0.0023\n",
      "Epoch 32/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0022\n",
      "Epoch 33/100\n",
      "997/997 [==============================] - 177s 177ms/step - loss: 0.0022\n",
      "Epoch 34/100\n",
      "997/997 [==============================] - 188s 188ms/step - loss: 0.0022\n",
      "Epoch 35/100\n",
      "997/997 [==============================] - 187s 188ms/step - loss: 0.0023\n",
      "Epoch 36/100\n",
      "997/997 [==============================] - 189s 190ms/step - loss: 0.0023\n",
      "Epoch 37/100\n",
      "997/997 [==============================] - 186s 187ms/step - loss: 0.0022\n",
      "Epoch 38/100\n",
      "997/997 [==============================] - 185s 185ms/step - loss: 0.0022\n",
      "Epoch 39/100\n",
      "997/997 [==============================] - 188s 189ms/step - loss: 0.0022\n",
      "Epoch 40/100\n",
      "997/997 [==============================] - 187s 188ms/step - loss: 0.0023\n",
      "Epoch 41/100\n",
      "997/997 [==============================] - 187s 187ms/step - loss: 0.0022\n",
      "Epoch 42/100\n",
      "997/997 [==============================] - 200s 200ms/step - loss: 0.0022\n",
      "Epoch 43/100\n",
      "997/997 [==============================] - 216s 216ms/step - loss: 0.0022\n",
      "Epoch 44/100\n",
      "997/997 [==============================] - 201s 201ms/step - loss: 0.0022\n",
      "Epoch 45/100\n",
      "997/997 [==============================] - 203s 204ms/step - loss: 0.0022\n",
      "Epoch 46/100\n",
      "997/997 [==============================] - 203s 204ms/step - loss: 0.0022\n",
      "Epoch 47/100\n",
      "997/997 [==============================] - 202s 203ms/step - loss: 0.0023\n",
      "Epoch 48/100\n",
      "997/997 [==============================] - 203s 204ms/step - loss: 0.0023\n",
      "Epoch 49/100\n",
      "997/997 [==============================] - 206s 206ms/step - loss: 0.0023\n",
      "Epoch 50/100\n",
      "997/997 [==============================] - 195s 196ms/step - loss: 0.0022\n",
      "Epoch 51/100\n",
      "997/997 [==============================] - 181s 182ms/step - loss: 0.0022\n",
      "Epoch 52/100\n",
      "997/997 [==============================] - 182s 183ms/step - loss: 0.0023\n",
      "Epoch 53/100\n",
      "997/997 [==============================] - 177s 178ms/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 55/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 56/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 57/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 59/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0027\n",
      "Epoch 60/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0022\n",
      "Epoch 61/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 62/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 63/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 64/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 65/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 66/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 67/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 68/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 69/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 70/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0023\n",
      "Epoch 71/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 72/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0023\n",
      "Epoch 73/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 74/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 75/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 76/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 77/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0023\n",
      "Epoch 78/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0023\n",
      "Epoch 79/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 80/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 81/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 82/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 83/100\n",
      "997/997 [==============================] - 160s 160ms/step - loss: 0.0023\n",
      "Epoch 84/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 85/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0021\n",
      "Epoch 86/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 87/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 88/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 89/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 90/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 91/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0021\n",
      "Epoch 92/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 93/100\n",
      "997/997 [==============================] - 161s 161ms/step - loss: 0.0022\n",
      "Epoch 94/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 96/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 97/100\n",
      "997/997 [==============================] - 162s 163ms/step - loss: 0.0023\n",
      "Epoch 98/100\n",
      "997/997 [==============================] - 161s 162ms/step - loss: 0.0022\n",
      "Epoch 99/100\n",
      "997/997 [==============================] - 160s 161ms/step - loss: 0.0022\n",
      "Epoch 100/100\n",
      "997/997 [==============================] - 163s 163ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbb69d6b08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(LSTM(units=500, return_sequences=True, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(LSTM(units=500, return_sequences=True))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(LSTM(units=500))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(units = 1))\n",
    "model4.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model4.fit(X_train_3d, y_train, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0002794508815626706"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions4 = model4.predict(X_test_3d)\n",
    "predictions4 = y_scaler.inverse_transform(predictions4)\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "model_score4 = r2_score(y_test[:-len(stock_dfs)*5], predictions4[:-len(stock_dfs)*5])\n",
    "model_score4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020785874447717956"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds4 = model4.predict(X_train_3d)\n",
    "train_preds4 = y_scaler.inverse_transform(train_preds4)\n",
    "y_train = y_scaler.inverse_transform(y_train)\n",
    "model_train_score4 = r2_score(y_train, train_preds4)\n",
    "model_train_score4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "drop_list = ['Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'Mt',\n",
    "       '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "       '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "       '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "       '5 Day Volume Var', '5 Day High', '5 Day Low',\n",
    "       '10 Day Open Mean', '10 Day High Mean', '10 Day Low Mean',\n",
    "       '10 Day Close Mean', '10 Day Volume Mean', '10 Day Open Var',\n",
    "       '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "       '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "       '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "       '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "       '20 Day Volume Var', '20 Day High', '20 Day Low']\n",
    "close_drop = ['Close']\n",
    "for day in range(1,31):\n",
    "    close_drop.append('- ' + str(day) + ' Days Close')\n",
    "for stock in stock_objects:\n",
    "    stock_dfs[stock] = stock_objects[stock].history(period='max')\n",
    "    stock_dfs[stock] = pf.rolling_aves(stock_dfs[stock])\n",
    "    stock_dfs[stock].drop(drop_list, axis=1, inplace=True)\n",
    "    stock_dfs[stock] = pf.lstm_prep(stock_dfs[stock], lookback=30)\n",
    "    stock_dfs[stock] = pf.future_percent_change_setup(stock_dfs[stock], 5)\n",
    "    stock_dfs[stock].drop(close_drop, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "\n",
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split(combine_df, 365, stock_dfs)\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.fit_transform(y_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 31, int(X_train.shape[1]/31)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 31, int(X_test.shape[1]/31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "997/997 [==============================] - 968s 968ms/step - loss: 0.0429\n",
      "Epoch 2/100\n",
      "997/997 [==============================] - 905s 908ms/step - loss: 0.0027\n",
      "Epoch 3/100\n",
      "997/997 [==============================] - 984s 987ms/step - loss: 0.0026\n",
      "Epoch 4/100\n",
      "997/997 [==============================] - 945s 948ms/step - loss: 0.0025\n",
      "Epoch 5/100\n",
      "997/997 [==============================] - 944s 947ms/step - loss: 0.0024\n",
      "Epoch 6/100\n",
      "997/997 [==============================] - 912s 915ms/step - loss: 0.0025\n",
      "Epoch 7/100\n",
      "997/997 [==============================] - 919s 922ms/step - loss: 0.0024\n",
      "Epoch 8/100\n",
      "997/997 [==============================] - 947s 950ms/step - loss: 0.0024\n",
      "Epoch 9/100\n",
      "997/997 [==============================] - 906s 909ms/step - loss: 0.0023\n",
      "Epoch 10/100\n",
      "997/997 [==============================] - 892s 894ms/step - loss: 0.0024\n",
      "Epoch 11/100\n",
      "997/997 [==============================] - 841s 844ms/step - loss: 0.0023\n",
      "Epoch 12/100\n",
      "997/997 [==============================] - 829s 832ms/step - loss: 0.0023\n",
      "Epoch 13/100\n",
      "997/997 [==============================] - 828s 831ms/step - loss: 0.0023\n",
      "Epoch 14/100\n",
      "997/997 [==============================] - 825s 828ms/step - loss: 0.0023\n",
      "Epoch 15/100\n",
      "997/997 [==============================] - 824s 826ms/step - loss: 0.0024\n",
      "Epoch 16/100\n",
      "997/997 [==============================] - 824s 827ms/step - loss: 0.0140\n",
      "Epoch 17/100\n",
      "997/997 [==============================] - 825s 827ms/step - loss: 0.0023\n",
      "Epoch 18/100\n",
      "997/997 [==============================] - 830s 832ms/step - loss: 0.0023\n",
      "Epoch 19/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 20/100\n",
      "997/997 [==============================] - 824s 827ms/step - loss: 0.0022\n",
      "Epoch 21/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0021\n",
      "Epoch 22/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 23/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 24/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 25/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 26/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 27/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 28/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0023\n",
      "Epoch 29/100\n",
      "997/997 [==============================] - 824s 826ms/step - loss: 0.0024\n",
      "Epoch 30/100\n",
      "997/997 [==============================] - 822s 824ms/step - loss: 0.0022\n",
      "Epoch 31/100\n",
      "997/997 [==============================] - 822s 824ms/step - loss: 0.0022\n",
      "Epoch 32/100\n",
      "997/997 [==============================] - 822s 824ms/step - loss: 0.0021\n",
      "Epoch 33/100\n",
      "997/997 [==============================] - 821s 824ms/step - loss: 0.0022\n",
      "Epoch 34/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 35/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 36/100\n",
      "997/997 [==============================] - 827s 829ms/step - loss: 0.0022\n",
      "Epoch 37/100\n",
      "997/997 [==============================] - 830s 832ms/step - loss: 0.0022\n",
      "Epoch 38/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 39/100\n",
      "997/997 [==============================] - 825s 827ms/step - loss: 0.0022\n",
      "Epoch 40/100\n",
      "997/997 [==============================] - 822s 825ms/step - loss: 0.0022\n",
      "Epoch 41/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 42/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 43/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 44/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 45/100\n",
      "997/997 [==============================] - 822s 825ms/step - loss: 0.0026\n",
      "Epoch 46/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0023\n",
      "Epoch 47/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0023\n",
      "Epoch 48/100\n",
      "997/997 [==============================] - 822s 824ms/step - loss: 0.0022\n",
      "Epoch 49/100\n",
      "997/997 [==============================] - 822s 825ms/step - loss: 0.0023\n",
      "Epoch 50/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 51/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 52/100\n",
      "997/997 [==============================] - 823s 826ms/step - loss: 0.0022\n",
      "Epoch 53/100\n",
      "997/997 [==============================] - 822s 824ms/step - loss: 0.0022\n",
      "Epoch 54/100\n",
      "997/997 [==============================] - 880s 883ms/step - loss: 0.0022\n",
      "Epoch 55/100\n",
      "997/997 [==============================] - 1040s 1s/step - loss: 0.0022\n",
      "Epoch 56/100\n",
      "997/997 [==============================] - 1011s 1s/step - loss: 0.0023\n",
      "Epoch 57/100\n",
      "997/997 [==============================] - 1137s 1s/step - loss: 0.0022\n",
      "Epoch 58/100\n",
      "997/997 [==============================] - 1095s 1s/step - loss: 0.0022\n",
      "Epoch 59/100\n",
      "997/997 [==============================] - 985s 988ms/step - loss: 0.0022\n",
      "Epoch 60/100\n",
      "997/997 [==============================] - 1017s 1s/step - loss: 0.0022\n",
      "Epoch 61/100\n",
      "997/997 [==============================] - 1052s 1s/step - loss: 0.0022\n",
      "Epoch 62/100\n",
      "997/997 [==============================] - 969s 971ms/step - loss: 0.0022\n",
      "Epoch 63/100\n",
      "997/997 [==============================] - 861s 863ms/step - loss: 0.0022\n",
      "Epoch 64/100\n",
      "997/997 [==============================] - 887s 889ms/step - loss: 0.0022\n",
      "Epoch 65/100\n",
      "997/997 [==============================] - 883s 886ms/step - loss: 0.0022\n",
      "Epoch 66/100\n",
      "997/997 [==============================] - 828s 831ms/step - loss: 0.0022\n",
      "Epoch 67/100\n",
      "997/997 [==============================] - 824s 827ms/step - loss: 0.0022\n",
      "Epoch 68/100\n",
      "997/997 [==============================] - 826s 828ms/step - loss: 0.0022\n",
      "Epoch 69/100\n",
      "997/997 [==============================] - 823s 825ms/step - loss: 0.0022\n",
      "Epoch 70/100\n",
      "997/997 [==============================] - 822s 825ms/step - loss: 0.0022\n",
      "Epoch 71/100\n",
      "997/997 [==============================] - 825s 827ms/step - loss: 0.0022\n",
      "Epoch 72/100\n",
      "997/997 [==============================] - 905s 908ms/step - loss: 0.0022\n",
      "Epoch 73/100\n",
      "997/997 [==============================] - 829s 832ms/step - loss: 0.0023\n",
      "Epoch 74/100\n",
      "997/997 [==============================] - 971s 974ms/step - loss: 0.0022\n",
      "Epoch 75/100\n",
      "997/997 [==============================] - 927s 930ms/step - loss: 0.0023\n",
      "Epoch 76/100\n",
      "997/997 [==============================] - 904s 907ms/step - loss: 0.0023\n",
      "Epoch 77/100\n",
      "997/997 [==============================] - 867s 869ms/step - loss: 0.0022\n",
      "Epoch 78/100\n",
      "997/997 [==============================] - 896s 899ms/step - loss: 0.0023\n",
      "Epoch 79/100\n",
      "997/997 [==============================] - 869s 872ms/step - loss: 0.0022\n",
      "Epoch 80/100\n",
      "997/997 [==============================] - 879s 881ms/step - loss: 0.0023\n",
      "Epoch 81/100\n",
      "997/997 [==============================] - 882s 885ms/step - loss: 0.0023\n",
      "Epoch 82/100\n",
      "997/997 [==============================] - 928s 931ms/step - loss: 0.0022\n",
      "Epoch 83/100\n",
      "997/997 [==============================] - 977s 980ms/step - loss: 0.0022\n",
      "Epoch 84/100\n",
      "997/997 [==============================] - 1030s 1s/step - loss: 0.0022\n",
      "Epoch 85/100\n",
      "997/997 [==============================] - 1043s 1s/step - loss: 0.0022\n",
      "Epoch 86/100\n",
      "997/997 [==============================] - 878s 880ms/step - loss: 0.0022\n",
      "Epoch 87/100\n",
      "997/997 [==============================] - 886s 889ms/step - loss: 0.0022\n",
      "Epoch 88/100\n",
      "997/997 [==============================] - 926s 929ms/step - loss: 0.0023\n",
      "Epoch 89/100\n",
      "997/997 [==============================] - 931s 934ms/step - loss: 0.0023\n",
      "Epoch 90/100\n",
      "997/997 [==============================] - 891s 894ms/step - loss: 0.0023\n",
      "Epoch 91/100\n",
      "997/997 [==============================] - 884s 887ms/step - loss: 0.0022\n",
      "Epoch 92/100\n",
      "997/997 [==============================] - 871s 874ms/step - loss: 0.0022\n",
      "Epoch 93/100\n",
      "997/997 [==============================] - 846s 849ms/step - loss: 0.0023\n",
      "Epoch 94/100\n",
      "997/997 [==============================] - 824s 826ms/step - loss: 0.0022\n",
      "Epoch 95/100\n",
      "997/997 [==============================] - 824s 826ms/step - loss: 0.0023\n",
      "Epoch 96/100\n",
      "997/997 [==============================] - 869s 871ms/step - loss: 0.0022\n",
      "Epoch 97/100\n",
      "997/997 [==============================] - 868s 871ms/step - loss: 0.0022\n",
      "Epoch 98/100\n",
      "997/997 [==============================] - 869s 872ms/step - loss: 0.0023\n",
      "Epoch 99/100\n",
      "997/997 [==============================] - 825s 828ms/step - loss: 0.0022\n",
      "Epoch 100/100\n",
      "997/997 [==============================] - 932s 935ms/step - loss: 0.0023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbbea34488>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(LSTM(units=500, return_sequences=True, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(LSTM(units=500, return_sequences=True))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(LSTM(units=500))\n",
    "model5.add(Dropout(0.2))\n",
    "model5.add(Dense(units = 1))\n",
    "model5.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model5.fit(X_train_3d, y_train, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.007025678225370147"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions5 = model5.predict(X_test_3d)\n",
    "predictions5 = y_scaler.inverse_transform(predictions5)\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "model_score5 = r2_score(y_test[:-len(stock_dfs)*5], predictions5[:-len(stock_dfs)*5])\n",
    "model_score5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0007052570995516039"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds5 = model5.predict(X_train_3d)\n",
    "train_preds5 = y_scaler.inverse_transform(train_preds5)\n",
    "y_train = y_scaler.inverse_transform(y_train)\n",
    "model_train_score5 = r2_score(y_train, train_preds5)\n",
    "model_train_score5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Change 5 and 30 day lookback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "drop_list = ['Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'Mt',\n",
    "       '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "       '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "       '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "       '5 Day Volume Var', '5 Day High', '5 Day Low',\n",
    "       '10 Day Open Mean', '10 Day High Mean', '10 Day Low Mean',\n",
    "       '10 Day Close Mean', '10 Day Volume Mean', '10 Day Open Var',\n",
    "       '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "       '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "       '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "       '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "       '20 Day Volume Var', '20 Day High', '20 Day Low']\n",
    "close_drop = ['Close']\n",
    "for day in range(1,6):\n",
    "    close_drop.append('- ' + str(day) + ' Days Close')\n",
    "for stock in stock_objects:\n",
    "    stock_dfs[stock] = stock_objects[stock].history(period='max')\n",
    "    stock_dfs[stock] = pf.rolling_aves(stock_dfs[stock])\n",
    "    #stock_dfs[stock].drop(drop_list, axis=1, inplace=True)\n",
    "    stock_dfs[stock] = pf.lstm_prep(stock_dfs[stock], lookback=5)\n",
    "    stock_dfs[stock] = pf.future_close_setup(stock_dfs[stock], 5)\n",
    "    #stock_dfs[stock].drop(close_drop, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "\n",
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split(combine_df, 365, stock_dfs)\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.fit_transform(y_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 6, int(X_train.shape[1]/6)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 6, int(X_test.shape[1]/6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "997/997 [==============================] - 192s 189ms/step - loss: 0.0015\n",
      "Epoch 2/100\n",
      "997/997 [==============================] - 191s 191ms/step - loss: 2.9206e-04\n",
      "Epoch 3/100\n",
      "997/997 [==============================] - 191s 191ms/step - loss: 2.6524e-04\n",
      "Epoch 4/100\n",
      "997/997 [==============================] - 189s 190ms/step - loss: 1.9889e-04\n",
      "Epoch 5/100\n",
      "997/997 [==============================] - 190s 191ms/step - loss: 2.0845e-04\n",
      "Epoch 6/100\n",
      "997/997 [==============================] - 188s 188ms/step - loss: 2.1243e-04s - loss: 2.1247e-\n",
      "Epoch 7/100\n",
      "997/997 [==============================] - 187s 188ms/step - loss: 2.0688e-04s - loss: 2.0692e\n",
      "Epoch 8/100\n",
      "997/997 [==============================] - 187s 188ms/step - loss: 1.4779e-04\n",
      "Epoch 9/100\n",
      "997/997 [==============================] - 188s 188ms/step - loss: 2.2380e-04\n",
      "Epoch 10/100\n",
      "997/997 [==============================] - 189s 190ms/step - loss: 1.4860e-04\n",
      "Epoch 11/100\n",
      "997/997 [==============================] - 191s 192ms/step - loss: 1.4907e-04\n",
      "Epoch 12/100\n",
      "997/997 [==============================] - 188s 188ms/step - loss: 1.5405e-04\n",
      "Epoch 13/100\n",
      "997/997 [==============================] - 183s 183ms/step - loss: 1.1977e-04\n",
      "Epoch 14/100\n",
      "997/997 [==============================] - 185s 185ms/step - loss: 1.6342e-04 ETA: 2s - lo\n",
      "Epoch 15/100\n",
      "997/997 [==============================] - 187s 187ms/step - loss: 1.2939e-04\n",
      "Epoch 16/100\n",
      "997/997 [==============================] - 188s 188ms/step - loss: 1.3782e-04\n",
      "Epoch 17/100\n",
      "997/997 [==============================] - 202s 203ms/step - loss: 1.2098e-04\n",
      "Epoch 18/100\n",
      "997/997 [==============================] - 196s 197ms/step - loss: 1.1588e-04\n",
      "Epoch 19/100\n",
      "997/997 [==============================] - 207s 208ms/step - loss: 1.3078e-04\n",
      "Epoch 20/100\n",
      "997/997 [==============================] - 206s 206ms/step - loss: 1.0570e-04\n",
      "Epoch 21/100\n",
      "997/997 [==============================] - 188s 189ms/step - loss: 1.0979e-04\n",
      "Epoch 22/100\n",
      "997/997 [==============================] - 186s 186ms/step - loss: 1.0314e-04\n",
      "Epoch 23/100\n",
      "997/997 [==============================] - 187s 187ms/step - loss: 9.6179e-05\n",
      "Epoch 24/100\n",
      "997/997 [==============================] - 183s 183ms/step - loss: 1.1546e-04\n",
      "Epoch 25/100\n",
      "997/997 [==============================] - 196s 196ms/step - loss: 1.0740e-04\n",
      "Epoch 26/100\n",
      "997/997 [==============================] - 187s 188ms/step - loss: 1.2438e-04s - loss: 1.24\n",
      "Epoch 27/100\n",
      "997/997 [==============================] - 185s 186ms/step - loss: 1.1224e-04\n",
      "Epoch 28/100\n",
      "997/997 [==============================] - 187s 187ms/step - loss: 9.6026e-05\n",
      "Epoch 29/100\n",
      "997/997 [==============================] - 188s 188ms/step - loss: 9.9611e-05\n",
      "Epoch 30/100\n",
      "997/997 [==============================] - 178s 179ms/step - loss: 1.1103e-04\n",
      "Epoch 31/100\n",
      "997/997 [==============================] - 176s 176ms/step - loss: 1.0280e-04\n",
      "Epoch 32/100\n",
      "997/997 [==============================] - 179s 180ms/step - loss: 8.3418e-05\n",
      "Epoch 33/100\n",
      "997/997 [==============================] - 182s 182ms/step - loss: 8.2722e-05\n",
      "Epoch 34/100\n",
      "997/997 [==============================] - 213s 214ms/step - loss: 7.6066e-05\n",
      "Epoch 35/100\n",
      "997/997 [==============================] - 226s 227ms/step - loss: 8.0963e-05\n",
      "Epoch 36/100\n",
      "997/997 [==============================] - 218s 219ms/step - loss: 8.6490e-05\n",
      "Epoch 37/100\n",
      "997/997 [==============================] - 225s 226ms/step - loss: 8.4407e-05\n",
      "Epoch 38/100\n",
      "997/997 [==============================] - 221s 222ms/step - loss: 9.8615e-05\n",
      "Epoch 39/100\n",
      "997/997 [==============================] - 226s 226ms/step - loss: 8.0377e-05\n",
      "Epoch 40/100\n",
      "997/997 [==============================] - 225s 226ms/step - loss: 8.1607e-05\n",
      "Epoch 41/100\n",
      "997/997 [==============================] - 222s 222ms/step - loss: 7.4656e-05\n",
      "Epoch 42/100\n",
      "997/997 [==============================] - 225s 226ms/step - loss: 7.4476e-05\n",
      "Epoch 43/100\n",
      "997/997 [==============================] - 231s 231ms/step - loss: 7.6750e-05\n",
      "Epoch 44/100\n",
      "997/997 [==============================] - 225s 226ms/step - loss: 7.0256e-05\n",
      "Epoch 45/100\n",
      "997/997 [==============================] - 239s 240ms/step - loss: 8.4032e-05\n",
      "Epoch 46/100\n",
      "997/997 [==============================] - 239s 240ms/step - loss: 8.6738e-05\n",
      "Epoch 47/100\n",
      "997/997 [==============================] - 251s 252ms/step - loss: 7.4453e-05\n",
      "Epoch 48/100\n",
      "997/997 [==============================] - 250s 251ms/step - loss: 7.2098e-05\n",
      "Epoch 49/100\n",
      "997/997 [==============================] - 244s 245ms/step - loss: 7.4616e-05\n",
      "Epoch 50/100\n",
      "997/997 [==============================] - 203s 203ms/step - loss: 7.2922e-05s - loss: 7.2\n",
      "Epoch 51/100\n",
      "997/997 [==============================] - 183s 183ms/step - loss: 7.2513e-05\n",
      "Epoch 52/100\n",
      "997/997 [==============================] - 183s 184ms/step - loss: 7.3748e-05s - loss: 7.37\n",
      "Epoch 53/100\n",
      "997/997 [==============================] - 183s 184ms/step - loss: 7.7383e-05 - ETA: 0s - loss: 7.7390e-\n",
      "Epoch 54/100\n",
      "997/997 [==============================] - 183s 183ms/step - loss: 6.1751e-05\n",
      "Epoch 55/100\n",
      "997/997 [==============================] - 183s 184ms/step - loss: 6.8173e-05\n",
      "Epoch 56/100\n",
      "997/997 [==============================] - 183s 183ms/step - loss: 7.5789e-05\n",
      "Epoch 57/100\n",
      "997/997 [==============================] - 183s 184ms/step - loss: 7.5346e-05\n",
      "Epoch 58/100\n",
      "997/997 [==============================] - 173s 173ms/step - loss: 6.5489e-05\n",
      "Epoch 59/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 7.3100e-05\n",
      "Epoch 60/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 6.4161e-05\n",
      "Epoch 61/100\n",
      "997/997 [==============================] - 164s 164ms/step - loss: 7.2606e-05\n",
      "Epoch 62/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 6.5870e-05\n",
      "Epoch 63/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.9768e-05\n",
      "Epoch 64/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 7.1086e-05\n",
      "Epoch 65/100\n",
      "997/997 [==============================] - 164s 164ms/step - loss: 7.2018e-05\n",
      "Epoch 66/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 6.2889e-05\n",
      "Epoch 67/100\n",
      "997/997 [==============================] - 165s 166ms/step - loss: 6.4084e-05\n",
      "Epoch 68/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 7.7924e-05\n",
      "Epoch 69/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 7.2152e-05\n",
      "Epoch 70/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.5972e-05\n",
      "Epoch 71/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.4328e-05\n",
      "Epoch 72/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.5510e-05\n",
      "Epoch 73/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.8960e-05\n",
      "Epoch 74/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.6790e-05\n",
      "Epoch 75/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 7.9602e-05\n",
      "Epoch 76/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.7765e-05\n",
      "Epoch 77/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.9364e-05\n",
      "Epoch 78/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 6.2295e-05\n",
      "Epoch 79/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.1164e-05s - loss: \n",
      "Epoch 80/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.9910e-05\n",
      "Epoch 81/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.3548e-05\n",
      "Epoch 82/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.5486e-05\n",
      "Epoch 83/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.3150e-05\n",
      "Epoch 84/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 6.3957e-05\n",
      "Epoch 85/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.6013e-05\n",
      "Epoch 86/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.5376e-05s \n",
      "Epoch 87/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 6.1736e-05\n",
      "Epoch 88/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.9740e-05\n",
      "Epoch 89/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.1082e-05\n",
      "Epoch 90/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.4486e-05\n",
      "Epoch 91/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.6865e-05\n",
      "Epoch 92/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.7315e-05s - \n",
      "Epoch 93/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.6701e-05\n",
      "Epoch 94/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.6844e-05\n",
      "Epoch 95/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.3075e-05\n",
      "Epoch 96/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.2044e-05\n",
      "Epoch 97/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.7909e-05\n",
      "Epoch 98/100\n",
      "997/997 [==============================] - 165s 165ms/step - loss: 5.3245e-05\n",
      "Epoch 99/100\n",
      "997/997 [==============================] - 164s 164ms/step - loss: 5.3591e-05\n",
      "Epoch 100/100\n",
      "997/997 [==============================] - 164s 165ms/step - loss: 5.4898e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbb68a97c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(LSTM(units=500, return_sequences=True, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(LSTM(units=500, return_sequences=True))\n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(LSTM(units=500))\n",
    "model6.add(Dropout(0.2))\n",
    "model6.add(Dense(units = 1))\n",
    "model6.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model6.fit(X_train_3d, y_train, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597922184049694"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions6 = model6.predict(X_test_3d)\n",
    "predictions6 = y_scaler.inverse_transform(predictions6)\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "model_score6 = r2_score(y_test[:-len(stock_dfs)*5], predictions6[:-len(stock_dfs)*5])\n",
    "model_score6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984521684427576"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds6 = model6.predict(X_train_3d)\n",
    "train_preds6 = y_scaler.inverse_transform(train_preds6)\n",
    "y_train = y_scaler.inverse_transform(y_train)\n",
    "model_train_score6 = r2_score(y_train, train_preds6)\n",
    "model_train_score6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dfs = {}\n",
    "drop_list = ['Open', 'High', 'Low', 'Volume', 'Dividends', 'Stock Splits', 'Mt',\n",
    "       '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "       '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "       '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "       '5 Day Volume Var', '5 Day High', '5 Day Low',\n",
    "       '10 Day Open Mean', '10 Day High Mean', '10 Day Low Mean',\n",
    "       '10 Day Close Mean', '10 Day Volume Mean', '10 Day Open Var',\n",
    "       '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "       '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "       '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "       '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "       '20 Day Volume Var', '20 Day High', '20 Day Low']\n",
    "close_drop = ['Close']\n",
    "for day in range(1,30):\n",
    "    close_drop.append('- ' + str(day) + ' Days Close')\n",
    "for stock in stock_objects:\n",
    "    stock_dfs[stock] = stock_objects[stock].history(period='max')\n",
    "    stock_dfs[stock] = pf.rolling_aves(stock_dfs[stock])\n",
    "    #stock_dfs[stock].drop(drop_list, axis=1, inplace=True)\n",
    "    stock_dfs[stock] = pf.lstm_prep(stock_dfs[stock], lookback=30)\n",
    "    stock_dfs[stock] = pf.future_close_setup(stock_dfs[stock], 5)\n",
    "    #stock_dfs[stock].drop(close_drop, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "\n",
    "X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split(combine_df, 365, stock_dfs)\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "X_train = X_scaler.fit_transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_scaler.fit_transform(y_test)\n",
    "X_train_3d = np.reshape(X_train, (X_train.shape[0], 31, int(X_train.shape[1]/31)))\n",
    "X_test_3d = np.reshape(X_test, (X_test.shape[0], 31, int(X_test.shape[1]/31)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "997/997 [==============================] - 790s 789ms/step - loss: 0.0170\n",
      "Epoch 2/100\n",
      "997/997 [==============================] - 788s 790ms/step - loss: 2.4065e-04\n",
      "Epoch 3/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 2.0584e-04\n",
      "Epoch 4/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 2.0944e-04\n",
      "Epoch 5/100\n",
      "997/997 [==============================] - 788s 791ms/step - loss: 2.2622e-04\n",
      "Epoch 6/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 1.8847e-04\n",
      "Epoch 7/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 1.8591e-04\n",
      "Epoch 8/100\n",
      "997/997 [==============================] - 791s 793ms/step - loss: 1.6722e-04\n",
      "Epoch 9/100\n",
      "997/997 [==============================] - 789s 792ms/step - loss: 1.7976e-04\n",
      "Epoch 10/100\n",
      "997/997 [==============================] - 790s 793ms/step - loss: 1.8773e-04\n",
      "Epoch 11/100\n",
      "997/997 [==============================] - 791s 793ms/step - loss: 1.7115e-04\n",
      "Epoch 12/100\n",
      "997/997 [==============================] - 792s 795ms/step - loss: 1.6823e-04\n",
      "Epoch 13/100\n",
      "997/997 [==============================] - 790s 792ms/step - loss: 1.7189e-04\n",
      "Epoch 14/100\n",
      "997/997 [==============================] - 790s 792ms/step - loss: 1.7380e-04\n",
      "Epoch 15/100\n",
      "997/997 [==============================] - 798s 800ms/step - loss: 1.9087e-04\n",
      "Epoch 16/100\n",
      "997/997 [==============================] - 786s 788ms/step - loss: 1.4688e-04\n",
      "Epoch 17/100\n",
      "997/997 [==============================] - 786s 788ms/step - loss: 1.6162e-04\n",
      "Epoch 18/100\n",
      "997/997 [==============================] - 786s 788ms/step - loss: 1.6178e-04\n",
      "Epoch 19/100\n",
      "997/997 [==============================] - 787s 789ms/step - loss: 1.2140e-04\n",
      "Epoch 20/100\n",
      "997/997 [==============================] - 787s 789ms/step - loss: 1.1889e-04\n",
      "Epoch 21/100\n",
      "997/997 [==============================] - 788s 790ms/step - loss: 1.6944e-04\n",
      "Epoch 22/100\n",
      "997/997 [==============================] - 788s 790ms/step - loss: 1.1696e-04\n",
      "Epoch 23/100\n",
      "997/997 [==============================] - 788s 791ms/step - loss: 1.1632e-04\n",
      "Epoch 24/100\n",
      "997/997 [==============================] - 787s 789ms/step - loss: 1.0624e-04\n",
      "Epoch 25/100\n",
      "997/997 [==============================] - 787s 790ms/step - loss: 1.1045e-04\n",
      "Epoch 26/100\n",
      "997/997 [==============================] - 787s 790ms/step - loss: 1.0947e-04\n",
      "Epoch 27/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 1.1679e-04\n",
      "Epoch 28/100\n",
      "997/997 [==============================] - 789s 792ms/step - loss: 1.0688e-04\n",
      "Epoch 29/100\n",
      "997/997 [==============================] - 787s 789ms/step - loss: 1.1599e-04\n",
      "Epoch 30/100\n",
      "997/997 [==============================] - 789s 792ms/step - loss: 8.4822e-05\n",
      "Epoch 31/100\n",
      "997/997 [==============================] - 787s 790ms/step - loss: 1.0039e-04\n",
      "Epoch 32/100\n",
      "997/997 [==============================] - 831s 834ms/step - loss: 9.2024e-05\n",
      "Epoch 33/100\n",
      "997/997 [==============================] - 875s 877ms/step - loss: 8.6085e-05\n",
      "Epoch 34/100\n",
      "997/997 [==============================] - 1043s 1s/step - loss: 8.8046e-05\n",
      "Epoch 35/100\n",
      "997/997 [==============================] - 1030s 1s/step - loss: 8.4807e-05\n",
      "Epoch 36/100\n",
      "997/997 [==============================] - 996s 999ms/step - loss: 8.7072e-05\n",
      "Epoch 37/100\n",
      "997/997 [==============================] - 970s 973ms/step - loss: 9.7073e-05\n",
      "Epoch 38/100\n",
      "997/997 [==============================] - 951s 954ms/step - loss: 8.2110e-05\n",
      "Epoch 39/100\n",
      "997/997 [==============================] - 987s 990ms/step - loss: 7.7395e-05\n",
      "Epoch 40/100\n",
      "997/997 [==============================] - 989s 992ms/step - loss: 7.9924e-05\n",
      "Epoch 41/100\n",
      "997/997 [==============================] - 913s 916ms/step - loss: 8.2692e-05\n",
      "Epoch 42/100\n",
      "997/997 [==============================] - 847s 849ms/step - loss: 8.5963e-05\n",
      "Epoch 43/100\n",
      "997/997 [==============================] - 850s 852ms/step - loss: 7.0949e-05\n",
      "Epoch 44/100\n",
      "997/997 [==============================] - 852s 854ms/step - loss: 7.7619e-05\n",
      "Epoch 45/100\n",
      "997/997 [==============================] - 892s 895ms/step - loss: 7.0680e-05\n",
      "Epoch 46/100\n",
      "997/997 [==============================] - 892s 894ms/step - loss: 8.1848e-05\n",
      "Epoch 47/100\n",
      "997/997 [==============================] - 886s 889ms/step - loss: 7.3251e-05\n",
      "Epoch 48/100\n",
      "997/997 [==============================] - 885s 888ms/step - loss: 7.7836e-05\n",
      "Epoch 49/100\n",
      "997/997 [==============================] - 858s 861ms/step - loss: 7.0627e-05\n",
      "Epoch 50/100\n",
      "997/997 [==============================] - 936s 939ms/step - loss: 7.5801e-05\n",
      "Epoch 51/100\n",
      "997/997 [==============================] - 872s 875ms/step - loss: 6.7630e-05\n",
      "Epoch 52/100\n",
      "997/997 [==============================] - 793s 796ms/step - loss: 7.7596e-05\n",
      "Epoch 53/100\n",
      "997/997 [==============================] - 888s 891ms/step - loss: 6.5455e-05\n",
      "Epoch 54/100\n",
      "997/997 [==============================] - 896s 899ms/step - loss: 7.2608e-05\n",
      "Epoch 55/100\n",
      "997/997 [==============================] - 873s 876ms/step - loss: 6.4592e-05\n",
      "Epoch 56/100\n",
      "997/997 [==============================] - 872s 875ms/step - loss: 6.6929e-05\n",
      "Epoch 57/100\n",
      "997/997 [==============================] - 802s 804ms/step - loss: 6.4971e-05\n",
      "Epoch 58/100\n",
      "997/997 [==============================] - 796s 799ms/step - loss: 7.1190e-05\n",
      "Epoch 59/100\n",
      "997/997 [==============================] - 835s 838ms/step - loss: 6.1699e-05\n",
      "Epoch 60/100\n",
      "997/997 [==============================] - 800s 803ms/step - loss: 7.0255e-05\n",
      "Epoch 61/100\n",
      "997/997 [==============================] - 783s 786ms/step - loss: 6.7189e-05\n",
      "Epoch 62/100\n",
      "997/997 [==============================] - 792s 795ms/step - loss: 6.7922e-05\n",
      "Epoch 63/100\n",
      "997/997 [==============================] - 792s 795ms/step - loss: 6.4232e-05\n",
      "Epoch 64/100\n",
      "997/997 [==============================] - 793s 795ms/step - loss: 5.9070e-05\n",
      "Epoch 65/100\n",
      "997/997 [==============================] - 795s 797ms/step - loss: 6.1589e-05\n",
      "Epoch 66/100\n",
      "997/997 [==============================] - 832s 834ms/step - loss: 6.4027e-05\n",
      "Epoch 67/100\n",
      "997/997 [==============================] - 880s 882ms/step - loss: 5.9392e-05\n",
      "Epoch 68/100\n",
      "997/997 [==============================] - 881s 884ms/step - loss: 6.1848e-05\n",
      "Epoch 69/100\n",
      "997/997 [==============================] - 894s 897ms/step - loss: 6.0748e-05\n",
      "Epoch 70/100\n",
      "997/997 [==============================] - 886s 888ms/step - loss: 5.8052e-05\n",
      "Epoch 71/100\n",
      "997/997 [==============================] - 924s 927ms/step - loss: 5.8374e-05\n",
      "Epoch 72/100\n",
      "997/997 [==============================] - 839s 841ms/step - loss: 5.6665e-05\n",
      "Epoch 73/100\n",
      "997/997 [==============================] - 800s 802ms/step - loss: 7.3759e-05\n",
      "Epoch 74/100\n",
      "997/997 [==============================] - 795s 798ms/step - loss: 5.4459e-05\n",
      "Epoch 75/100\n",
      "997/997 [==============================] - 793s 795ms/step - loss: 6.4009e-05\n",
      "Epoch 76/100\n",
      "997/997 [==============================] - 794s 796ms/step - loss: 6.0220e-05\n",
      "Epoch 77/100\n",
      "997/997 [==============================] - 795s 798ms/step - loss: 5.7498e-05\n",
      "Epoch 78/100\n",
      "997/997 [==============================] - 795s 797ms/step - loss: 6.2526e-05\n",
      "Epoch 79/100\n",
      "997/997 [==============================] - 793s 795ms/step - loss: 5.1594e-05\n",
      "Epoch 80/100\n",
      "997/997 [==============================] - 793s 796ms/step - loss: 5.3838e-05\n",
      "Epoch 81/100\n",
      "997/997 [==============================] - 790s 793ms/step - loss: 5.1016e-05\n",
      "Epoch 82/100\n",
      "997/997 [==============================] - 875s 877ms/step - loss: 5.7413e-05\n",
      "Epoch 83/100\n",
      "997/997 [==============================] - 809s 811ms/step - loss: 5.6874e-05\n",
      "Epoch 84/100\n",
      "997/997 [==============================] - 846s 849ms/step - loss: 5.7700e-05\n",
      "Epoch 85/100\n",
      "997/997 [==============================] - 899s 901ms/step - loss: 5.5745e-05\n",
      "Epoch 86/100\n",
      "997/997 [==============================] - 875s 878ms/step - loss: 5.1387e-05\n",
      "Epoch 87/100\n",
      "997/997 [==============================] - 914s 916ms/step - loss: 5.4871e-05\n",
      "Epoch 88/100\n",
      "997/997 [==============================] - 861s 864ms/step - loss: 5.1944e-05\n",
      "Epoch 89/100\n",
      "997/997 [==============================] - 817s 819ms/step - loss: 5.7745e-05\n",
      "Epoch 90/100\n",
      "997/997 [==============================] - 1044s 1s/step - loss: 5.3998e-05\n",
      "Epoch 91/100\n",
      "997/997 [==============================] - 1146s 1s/step - loss: 5.8033e-05\n",
      "Epoch 92/100\n",
      "997/997 [==============================] - 1184s 1s/step - loss: 5.1522e-05\n",
      "Epoch 93/100\n",
      "997/997 [==============================] - 863s 865ms/step - loss: 5.1542e-05\n",
      "Epoch 94/100\n",
      "997/997 [==============================] - 790s 792ms/step - loss: 5.5290e-05\n",
      "Epoch 95/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 5.0480e-05\n",
      "Epoch 96/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 4.7008e-05\n",
      "Epoch 97/100\n",
      "997/997 [==============================] - 788s 790ms/step - loss: 5.0448e-05\n",
      "Epoch 98/100\n",
      "997/997 [==============================] - 788s 791ms/step - loss: 4.9102e-05\n",
      "Epoch 99/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 4.9557e-05\n",
      "Epoch 100/100\n",
      "997/997 [==============================] - 789s 791ms/step - loss: 5.2302e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbb7612dc8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(LSTM(units=500, return_sequences=True, activation=\"tanh\", input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model7.add(Dropout(0.2))\n",
    "model7.add(LSTM(units=500, return_sequences=True))\n",
    "model7.add(Dropout(0.2))\n",
    "model7.add(LSTM(units=500))\n",
    "model7.add(Dropout(0.2))\n",
    "model7.add(Dense(units = 1))\n",
    "model7.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "model7.fit(X_train_3d, y_train, epochs=100, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5751908966353667"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions7 = model7.predict(X_test_3d)\n",
    "predictions7 = y_scaler.inverse_transform(predictions7)\n",
    "y_test = y_scaler.inverse_transform(y_test)\n",
    "model_score7 = r2_score(y_test[:-len(stock_dfs)*5], predictions7[:-len(stock_dfs)*5])\n",
    "model_score7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984745680192805"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds7 = model7.predict(X_train_3d)\n",
    "train_preds7 = y_scaler.inverse_transform(train_preds7)\n",
    "y_train = y_scaler.inverse_transform(y_train)\n",
    "model_train_score7 = r2_score(y_train, train_preds7)\n",
    "model_train_score7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

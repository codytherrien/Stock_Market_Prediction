{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import datetime\n",
    "import project_functions2 as pf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list = ['AMZN', 'AAPL', 'FB','GOOG', 'MSFT', 'TSLA']\n",
    "stock_objects = {}\n",
    "for stock in stock_list:\n",
    "    stock_objects[stock] = yf.Ticker(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_sim_NN(stock_objects, split_time, time_shift):\n",
    "    scaler = MinMaxScaler()\n",
    "    columns = ['Date', 'Fund Value', 'Cash']\n",
    "    stock_names = []\n",
    "    curr_cash = 10000\n",
    "    curr_shares = {}\n",
    "    drop_list = [ 'Volume', 'Dividends', 'Stock Splits',\n",
    "                 '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "                 '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "                 '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "                 '5 Day Volume Var','10 Day Open Mean', '10 Day High Mean', \n",
    "                 '10 Day Low Mean','10 Day Close Mean', '10 Day Volume Mean', \n",
    "                 '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "                 '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "                 '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "                 '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "                 '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "                 '20 Day Volume Var', '10 Day Open Var']\n",
    "    \n",
    "    stock_dfs = {}\n",
    "    for key in stock_objects:\n",
    "        stock_dfs[key] = stock_objects[key].history(period='max')\n",
    "        \n",
    "    stock_investing = {}\n",
    "    for key in stock_objects:\n",
    "        stock_investing[key] = pd.read_csv('/content/drive/MyDrive/SENG474_Project/data/sentiment/investing_'+key+'_sentiment.csv')\n",
    "        stock_investing[key].set_index('date', inplace=True)\n",
    "    stock_stocks = {}\n",
    "    for key in stock_objects:\n",
    "        stock_stocks[key] = pd.read_csv('/content/drive/MyDrive/SENG474_Project/data/sentiment/stocks_'+key+'_sentiment.csv')\n",
    "        stock_stocks[key].set_index('date', inplace=True)\n",
    "    \n",
    "    for key in stock_dfs:\n",
    "        stock_dfs[key] = pf.rolling_aves(stock_dfs[key])\n",
    "        stock_dfs[key].drop(drop_list, axis=1, inplace=True)\n",
    "        stock_dfs[key] = stock_dfs[key].merge(stock_investing[key], how='left', left_index=True, right_index=True)\n",
    "        stock_dfs[key] = stock_dfs[key].merge(stock_stocks[key], how='left', left_index=True, right_index=True)\n",
    "        stock_dfs[key].fillna(0, inplace=True)\n",
    "        stock_dfs[key] = pf.future_close_setup(stock_dfs[key], 5)\n",
    "        \n",
    "    stock_low_dfs = {}\n",
    "    for key in stock_objects:\n",
    "        stock_low_dfs[key] = stock_objects[key].history(period='max')\n",
    "    \n",
    "    for key in stock_low_dfs:\n",
    "        stock_low_dfs[key] = pf.rolling_aves(stock_low_dfs[key])\n",
    "        stock_low_dfs[key].drop(drop_list, axis=1, inplace=True)\n",
    "        stock_low_dfs[key] = stock_low_dfs[key].merge(stock_investing[key], how='left', left_index=True, right_index=True)\n",
    "        stock_low_dfs[key] = stock_low_dfs[key].merge(stock_stocks[key], how='left', left_index=True, right_index=True)\n",
    "        stock_low_dfs[key].fillna(0, inplace=True)\n",
    "        stock_low_dfs[key] = pf.future_low_setup(stock_low_dfs[key], 1)\n",
    "        \n",
    "    combine_df = pf.combiner(stock_dfs)\n",
    "    cobine_low_df = pf.combiner(stock_low_dfs)\n",
    "        \n",
    "    test_dfs = stock_dfs\n",
    "    \n",
    "    for key in test_dfs:\n",
    "        test_dfs[key] = test_dfs[key].tail(split_time)\n",
    "        curr_shares[key] = 0\n",
    "        stock_names.append(key)\n",
    "    \n",
    "    columns = columns + stock_names\n",
    "    cash_df = pd.DataFrame(columns=columns)\n",
    "    curr_line = [combine_df.index[int(len(combine_df) - (split_time * len(stock_dfs)))], curr_cash, curr_cash] + len(stock_names)*[0]\n",
    "    cash_df.loc[len(cash_df)] = curr_line\n",
    "    \n",
    "    while split_time >= time_shift:\n",
    "        max_stock = ''\n",
    "        max_stock_gain = 0\n",
    "        X_train, y_train, X_test, y_test = pf.multi_stock_train_test_split(combine_df, split_time, stock_dfs)\n",
    "        X_train, low_train, X_test, low_test = pf.multi_stock_train_test_split(combine_low_df, split_time, stock_low_dfs)\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        stock_model = Sequential()\n",
    "        stock_model.add(Dense(units=150, input_dim=X_train_scaled.shape[1], activation=leaky_relu))\n",
    "        stock_model.add(Dense(units=1, activation=leaky_relu))\n",
    "        stock_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        stock_model.fit(X_train_scaled, y_train, epochs=60, batch_size=32, verbose=1,\n",
    "                        workers=-1, callbacks=[early_stopping])\n",
    "        low_model = Sequential()\n",
    "        low_model.add(Dense(units=150, input_dim=X_train_scaled.shape[1], activation=leaky_relu))\n",
    "        low_model.add(Dense(units=1, activation=leaky_relu))\n",
    "        low_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        low_model.fit(X_train_scaled, low_train, epochs=60, batch_size=32, verbose=1,\n",
    "                      workers=-1, callbacks=[early_stopping])\n",
    "        \n",
    "        for key in test_dfs:\n",
    "            X = test_dfs[key].iloc[:,:-1]\n",
    "            X = scaler.transform(X)\n",
    "            stock_pred = stock_model.predict(X.head(1))\n",
    "            stock_pred = (float(stock_pred) - X['Close'][0]) / X['Close'][0]\n",
    "            low_pred = low_model.predict(X.head(1))\n",
    "            if stock_pred > max_stock_gain:\n",
    "                max_stock = key\n",
    "                max_stock_gain = stock_pred\n",
    "                max_stock_low_pred = low_pred\n",
    "        \n",
    "        for key in test_dfs:\n",
    "            test_dfs[key] = test_dfs[key].iloc[1:]\n",
    "        \n",
    "        if max_stock_gain > 0:\n",
    "            proj_buy = float((test_dfs[max_stock]['Open'][0] - max_stock_low_pred)*0.5 + test_dfs[max_stock]['Low'][0])\n",
    "            #print(test_dfs[max_stock].index[0])\n",
    "            #if max_stock_low_pred >= test_dfs[max_stock]['Open'][0]:\n",
    "            #   print('Buy at open')\n",
    "            #print(float(proj_buy - test_dfs[max_stock]['Low'][0]))\n",
    "            if curr_shares[max_stock] == 0:\n",
    "                for key in curr_shares:\n",
    "                    curr_cash += curr_shares[key]*test_dfs[key]['Open'][0]\n",
    "                    curr_shares[key] = 0\n",
    "                if test_dfs[max_stock]['Open'][0] <= max_stock_low_pred and test_dfs[max_stock]['Open'][0] <= proj_buy:\n",
    "                    curr_shares[max_stock] = curr_cash // test_dfs[max_stock]['Open'][0]\n",
    "                    curr_cash -= curr_shares[max_stock]*test_dfs[max_stock]['Open'][0]\n",
    "                else:\n",
    "                    curr_shares[max_stock] = curr_cash // proj_buy\n",
    "                    curr_cash -= curr_shares[max_stock]*proj_buy\n",
    "            else:\n",
    "                if test_dfs[max_stock]['Open'][0] <= max_stock_low_pred and test_dfs[max_stock]['Open'][0] <= proj_buy:\n",
    "                    curr_shares[max_stock] += curr_cash // test_dfs[max_stock]['Open'][0]\n",
    "                    curr_cash -= (curr_cash // test_dfs[max_stock]['Open'][0]) * test_dfs[max_stock]['Open'][0]\n",
    "                else:\n",
    "                    curr_shares[max_stock] += curr_cash // proj_buy\n",
    "                    curr_cash -= (curr_cash // proj_buy) * proj_buy\n",
    "        else:\n",
    "            for key in curr_shares:\n",
    "                    curr_cash += curr_shares[key]*test_dfs[key]['Open'][0]\n",
    "                    curr_shares[key] = 0\n",
    "        \n",
    "        curr_line = [X_test.index[len(stock_names)], curr_cash ,curr_cash] + len(stock_names)*[0]\n",
    "        cash_df.loc[len(cash_df)] = curr_line\n",
    "        for key in curr_shares:\n",
    "            cash_df.iloc[-1, cash_df.columns.get_loc(key)] = curr_shares[key]\n",
    "            cash_df.iloc[-1, cash_df.columns.get_loc('Fund Value')] += curr_shares[key]*test_dfs[key]['Open'][0]\n",
    "        for key in test_dfs:\n",
    "            test_dfs[key] = test_dfs[key].iloc[time_shift-1:]\n",
    "        \n",
    "        split_time -= time_shift\n",
    "\n",
    "    return cash_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [ 'Volume', 'Dividends', 'Stock Splits',\n",
    "             '5 Day Open Mean', '5 Day High Mean', '5 Day Low Mean',\n",
    "             '5 Day Close Mean', '5 Day Volume Mean', '5 Day Open Var',\n",
    "             '5 Day High Var', '5 Day Low Var', '5 Day Close Var',\n",
    "             '5 Day Volume Var','10 Day Open Mean', '10 Day High Mean', \n",
    "             '10 Day Low Mean','10 Day Close Mean', '10 Day Volume Mean', \n",
    "             '10 Day High Var', '10 Day Low Var', '10 Day Close Var',\n",
    "             '10 Day Volume Var', '10 Day High', '10 Day Low', \n",
    "             '20 Day Open Mean', '20 Day High Mean', '20 Day Low Mean',\n",
    "             '20 Day Close Mean', '20 Day Volume Mean', '20 Day Open Var',\n",
    "             '20 Day High Var', '20 Day Low Var', '20 Day Close Var',\n",
    "             '20 Day Volume Var', '10 Day Open Var']\n",
    "\n",
    "stock_dfs = {}\n",
    "for key in stock_objects:\n",
    "    stock_dfs[key] = stock_objects[key].history(period='max')\n",
    "    \n",
    "#stock_investing = {}\n",
    "#for key in stock_objects:\n",
    "#    stock_investing[key] = pd.read_csv('/content/drive/MyDrive/SENG474_Project/data/sentiment/investing_'+key+'_sentiment.csv')\n",
    "#    stock_investing[key].set_index('date', inplace=True)\n",
    "#stock_stocks = {}\n",
    "#for key in stock_objects:\n",
    "#    stock_stocks[key] = pd.read_csv('/content/drive/MyDrive/SENG474_Project/data/sentiment/stocks_'+key+'_sentiment.csv')\n",
    "#    stock_stocks[key].set_index('date', inplace=True)\n",
    "\n",
    "for key in stock_dfs:\n",
    "    stock_dfs[key] = pf.rolling_aves(stock_dfs[key])\n",
    "    stock_dfs[key].drop(drop_list, axis=1, inplace=True)\n",
    "    #stock_dfs[key] = stock_dfs[key].merge(stock_investing[key], how='left', left_index=True, right_index=True)\n",
    "    #stock_dfs[key] = stock_dfs[key].merge(stock_stocks[key], how='left', left_index=True, right_index=True)\n",
    "    #stock_dfs[key].fillna(0, inplace=True)\n",
    "    stock_dfs[key] = pf.future_close_setup(stock_dfs[key], 5)\n",
    "    \n",
    "stock_low_dfs = {}\n",
    "for key in stock_objects:\n",
    "    stock_low_dfs[key] = stock_objects[key].history(period='max')\n",
    "\n",
    "for key in stock_low_dfs:\n",
    "    stock_low_dfs[key] = pf.rolling_aves(stock_low_dfs[key])\n",
    "    stock_low_dfs[key].drop(drop_list, axis=1, inplace=True)\n",
    "    #stock_low_dfs[key] = stock_low_dfs[key].merge(stock_investing[key], how='left', left_index=True, right_index=True)\n",
    "    #stock_low_dfs[key] = stock_low_dfs[key].merge(stock_stocks[key], how='left', left_index=True, right_index=True)\n",
    "    #stock_low_dfs[key].fillna(0, inplace=True)\n",
    "    stock_low_dfs[key] = pf.future_low_setup(stock_low_dfs[key], 1)\n",
    "combine_df = pf.combiner(stock_dfs)\n",
    "combine_low_df = pf.combiner(stock_low_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.74429803e-05, 1.75407197e-05, 1.78705376e-05, 1.76440318e-05,\n",
       "        1.76840270e-05, 1.22692547e-02, 1.69231385e-05, 1.84455435e-05,\n",
       "        1.76182130e-05, 0.00000000e+00, 1.73596433e-05, 0.00000000e+00,\n",
       "        1.46996153e-05, 1.95660575e-05, 1.71574565e-05, 0.00000000e+00,\n",
       "        1.36311618e-01, 1.74055500e-05, 1.73286031e-05, 1.72198261e-05,\n",
       "        4.94007001e-01, 5.10961496e-01, 4.29605496e-01, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.49934409e-16, 2.15998676e-01, 1.63596762e-01,\n",
       "        1.33464750e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.00059015e-01, 4.99381418e-01, 5.03327691e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = combine_df.iloc[:,:-1]\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled[:0:,].shape\n",
    "curr_x = X_scaled[:1:,]\n",
    "curr_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
